{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c56d78-c71e-4fa8-986d-a5d06a96ec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Reshape, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from itertools import islice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ec2f91-44ce-4d95-bfb1-9aa7fe50810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f7aefb-8265-4fb7-9e54-704979c233ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(file_path, pitch_length, pitch_width):\n",
    "   \n",
    "    # Read every 25 line starting from the 4th line of the dataset\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = list(csv.reader(islice(file, 3, None, 25)))\n",
    " \n",
    "    # Format the data into vectors of size 2 for x and y coordinates\n",
    "    processed_data = []\n",
    "    Ball_data = []\n",
    "    \n",
    "    for line in lines:\n",
    "        period, frame, time, *coordinates = line\n",
    "        p = int(period)\n",
    "        processed_coordinates = []\n",
    "        for i in range(0, len(coordinates), 2):\n",
    "            x, y = float(coordinates[i]), float(coordinates[i + 1])\n",
    "            if not math.isnan(x):\n",
    "                if p == 1:\n",
    "                    processed_coordinates.append([round(x * pitch_length, 2), round(y * pitch_width, 2)])\n",
    "                else:\n",
    "                    processed_coordinates.append([round((1-x) * pitch_length, 2), round(y * pitch_width, 2)])\n",
    "        Ball = processed_coordinates[-1]\n",
    "        processed_coordinates.pop()\n",
    "        new_line =  processed_coordinates\n",
    "        if len(new_line)==11:\n",
    "            last_coord = new_line[-1]\n",
    "        else:\n",
    "            new_line.append(last_coord)\n",
    "            \n",
    "        processed_data.append(new_line)\n",
    "        Ball_data.append(Ball)\n",
    "    \n",
    "    return processed_data, Ball_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa12387-0486-4369-8d88-656fd8b03c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions of the football pitch (in meters)\n",
    "pitch_length = 105.0\n",
    "pitch_width = 68.0\n",
    "\n",
    "file_path_Home = 'C:/Users/user/Notebooks/Soccer Valuation/Sample_Game_1_RawTrackingData_Home_Team.csv'\n",
    "file_path_Away = 'C:/Users/user/Notebooks/Soccer Valuation/Sample_Game_1_RawTrackingData_Away_Team.csv'\n",
    "Home_team, Ball_team = process_dataset(file_path_Home, pitch_length, pitch_width)\n",
    "Away_team, Ball_team = process_dataset(file_path_Away, pitch_length, pitch_width)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a8ba52-7dfa-4dc6-bcc2-6496b5a84d32",
   "metadata": {},
   "source": [
    "Compute States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29df1dc3-17f1-43a8-9b00-04dd9f9ecfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_element(matrix, element):\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix[0])):\n",
    "            if matrix[i][j] == element:\n",
    "                return np.array([i, j]) \n",
    "    return None \n",
    "    \n",
    "def counting(list, mat):\n",
    "    n=0\n",
    "    for matrix in list:       \n",
    "        if (matrix == mat).all():\n",
    "            n+=1\n",
    "    return n\n",
    "\n",
    "def find_matrix(list, mat):\n",
    "    index = 0\n",
    "    for matrix in list:\n",
    "        if (matrix == mat).all():\n",
    "            return index\n",
    "        index+=1\n",
    "\n",
    "def mirror_matrix(matrix):\n",
    "    mirrored_matrix = []\n",
    "    for row in matrix:\n",
    "        mirrored_row = list(reversed(row))  # Reverse the order of columns in the row\n",
    "        mirrored_matrix.append(mirrored_row)\n",
    "    return mirrored_matrix\n",
    "\n",
    "def place_in_compo(role):\n",
    "    role = torch.floor(torch.tensor(role/2))\n",
    "    i,j = role\n",
    "    return int(5*j + i)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe6907b7-0fe7-4684-a70f-ac837b0a46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_positions(players, goal):\n",
    "    n = len(players)\n",
    "    mat = torch.zeros(n,n)\n",
    "    sorted_v = sorted(players, key=lambda x: x[0])\n",
    "    sorted_h = sorted(players, key=lambda x: x[1])\n",
    "    k = 1\n",
    "    if goal==False:\n",
    "        k=2\n",
    "    for player in players:\n",
    "        i = sorted_h.index(player)\n",
    "        j = sorted_v.index(player)\n",
    "        if mat[i][j] != 0:\n",
    "            if mat[i+1][j+1] == 0:\n",
    "                mat[i+1][j+1] = k\n",
    "            elif mat[i+1][j-1] == 0:\n",
    "                mat[i+1][j-1] = k\n",
    "            elif mat[i-1][j-1] == 0:\n",
    "                mat[i-1][j-1] = k\n",
    "            elif mat[i-1][j+1] == 0:\n",
    "                mat[i-1][j+1] = k\n",
    "            else:\n",
    "                print(\"ERROR\")\n",
    "        else:\n",
    "            mat[i][j] = k\n",
    "        k+=1\n",
    "    return mat\n",
    "\n",
    "#This function gets the average team relative positions in a match\n",
    "def get_average_compo(Team):\n",
    "    Compositions = []\n",
    "    \n",
    "    for t in range(len(Team)):\n",
    "        In_field_players = Team[t][1:]\n",
    "        Compo = relative_positions(In_field_players, False)\n",
    "        Compositions.append(Compo)\n",
    "    \n",
    "    #Find the most frequent Composition\n",
    "    max = 0\n",
    "    index = 0\n",
    "    for comp in Compositions:\n",
    "        n = counting(Compositions, comp)\n",
    "        if n>= max:\n",
    "            max = n\n",
    "            index = find_matrix(Compositions, comp)\n",
    "        \n",
    "    average_compo = Compositions[index]\n",
    "    return average_compo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea0efdf5-b282-4b0b-bba2-8870ed14ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(player, Team, average_compo_team, rel_pos, t, Delta):\n",
    "\n",
    "    #First get indexes of the relative positions : Index_Game\n",
    "    index_player = find_element(rel_pos, player)\n",
    "    \n",
    "    # Now compute features   \n",
    "    #Team label\n",
    "    if player<=11:\n",
    "        team_label = 1\n",
    "    elif player<=22:\n",
    "        team_label = -1\n",
    "    else:\n",
    "        team_label = 23\n",
    "    \n",
    "    #Role_team\n",
    "    if team_label==1:\n",
    "        player -= 1\n",
    "    if team_label == -1:\n",
    "        player -= 12\n",
    "\n",
    "    Role_team = find_element(average_compo_team, player)\n",
    "    if Role_team is None:\n",
    "        Role_team = [0,0]\n",
    "\n",
    "\n",
    "    if player < 23:\n",
    "        #position\n",
    "        pos = Team[t][player]\n",
    "        #Velocity\n",
    "        if t>0:\n",
    "            vel = (torch.tensor(Team[t][player]) - torch.tensor(Team[t-1][player])) / Delta\n",
    "        else:\n",
    "            vel = [0,0]\n",
    "            vel = torch.tensor(vel)\n",
    "        #Acceleration\n",
    "        if t>1:\n",
    "            acc = (vel - ( torch.tensor(Team[t-1][player]) - torch.tensor(Team[t-2][player])) / Delta ) /Delta\n",
    "        else:\n",
    "            acc = [0,0]\n",
    "    else:\n",
    "        #position\n",
    "        pos = [Team[t]][0]\n",
    "        #Velocity\n",
    "        if t>0:\n",
    "            vel = (torch.tensor( [Team[t]][0]) - torch.tensor( [Team[t-1]][0])) / Delta\n",
    "        else:\n",
    "            vel = [0,0]\n",
    "            vel = torch.tensor(vel)\n",
    "        #Acceleration\n",
    "        if t>1:\n",
    "            acc = (vel - ( torch.tensor([Team[t-1]][0]) -  torch.tensor([Team[t-2]][0])) / Delta ) /Delta\n",
    "        else:\n",
    "            acc = [0,0]\n",
    "\n",
    "    return index_player, team_label, Role_team, pos, vel, acc\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "821ea66b-861a-48da-b2a2-1deaa946e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(Home, Away, Ball, average_compo_home, average_compo_away, t, Delta):\n",
    "\n",
    "    # Initialize state matrices\n",
    "    Mt = torch.zeros(23,23)\n",
    "    Mr1 = torch.zeros(23,23)\n",
    "    Mr2 = torch.zeros(23,23)\n",
    "    Mp1 = torch.zeros(23,23)\n",
    "    Mp2 = torch.zeros(23,23)\n",
    "    Mv1 = torch.zeros(23,23)\n",
    "    Mv2 = torch.zeros(23,23)\n",
    "    Ma1 = torch.zeros(23,23)\n",
    "    Ma2 = torch.zeros(23,23)\n",
    "   \n",
    "    Ball[t]\n",
    "    frame = Home[t] + Away[t] + [Ball[t]]\n",
    "    rel_pos = relative_positions(frame, True)\n",
    "    \n",
    "    for p in range(2,12):\n",
    "        index_player, team_label, Role_team, pos, vel, acc = get_features(p, Home, average_compo_home, rel_pos, t, Delta)\n",
    "        i,j = index_player\n",
    "        Mt[i][j] = team_label\n",
    "        Mr1[i][j] = Role_team[0] \n",
    "        Mr2[i][j] = Role_team[1] \n",
    "        Mp1[i][j] = pos[0] \n",
    "        Mp2[i][j] = pos[1] \n",
    "        Mv1[i][j] = vel[0] \n",
    "        Mv2[i][j] = vel[1] \n",
    "        Ma1[i][j] = acc[0] \n",
    "        Ma2[i][j] = acc[1] \n",
    "    for p in range(12,23):\n",
    "        index_player, team_label, Role_team, pos, vel, acc = get_features(p, Away, average_compo_home, rel_pos, t, Delta)\n",
    "        i,j = index_player\n",
    "        Mt[i][j] = team_label\n",
    "        Mr1[i][j] = Role_team[0] \n",
    "        Mr2[i][j] = Role_team[1] \n",
    "        Mp1[i][j] = pos[0] \n",
    "        Mp2[i][j] = pos[1] \n",
    "        Mv1[i][j] = vel[0] \n",
    "        Mv2[i][j] = vel[1] \n",
    "        Ma1[i][j] = acc[0] \n",
    "        Ma2[i][j] = acc[1] \n",
    "    p = 23\n",
    "    index_player, team_label, Role_team, pos, vel, acc = get_features(p, Ball, average_compo_home, rel_pos, t, Delta)\n",
    "    i,j = index_player\n",
    "    Mt[i][j] = team_label\n",
    "    Mr1[i][j] =i\n",
    "    Mr2[i][j] = j \n",
    "    Mp1[i][j] = pos[0] \n",
    "    Mp2[i][j] = pos[1] \n",
    "    Mv1[i][j] = vel[0] \n",
    "    Mv2[i][j] = vel[1] \n",
    "    Ma1[i][j] = acc[0] \n",
    "    Ma2[i][j] = acc[1] \n",
    "\n",
    "    \n",
    "    state = torch.stack([Mt, Mr1, Mr2, Mp1, Mp2, Mv1, Mv2, Ma1, Ma2], axis=0)\n",
    "    \n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f37b45a0-cde6-4a58-b544-a96dd5052929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Compute the average team relative positions in the match\\n\\naverage_compo_home = get_average_compo(Home_team)\\naverage_compo_away = get_average_compo(Away_team)\\naverage_compo_away = mirror_matrix(average_compo_away)\\n\\ntorch.save(average_compo_home, 'average_compo_home.pt')\\ntorch.save(average_compo_away, 'average_compo_away.pt')\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Compute the average team relative positions in the match\n",
    "\n",
    "average_compo_home = get_average_compo(Home_team)\n",
    "average_compo_away = get_average_compo(Away_team)\n",
    "average_compo_away = mirror_matrix(average_compo_away)\n",
    "\n",
    "torch.save(average_compo_home, 'average_compo_home.pt')\n",
    "torch.save(average_compo_away, 'average_compo_away.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "657e4fd4-cd86-4308-9c4e-e97af561e828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Compute the states\\nDelta = 1\\nMt = np.zeros((23, 23))\\nMr1 = np.zeros((23, 23))\\nMr2 = np.zeros((23, 23))\\nMp1 = np.zeros((23, 23))\\nMp2 = np.zeros((23, 23))\\nMv1 = np.zeros((23, 23))\\nMv2 = np.zeros((23, 23))\\nMa1 = np.zeros((23, 23))\\nMa2 = np.zeros((23, 23))\\nstate = np.stack([Mt,Mr1, Mr2, Mp1, Mp2, Mv1, Mv2, Ma1, Ma2], axis = 2)\\nStates = []\\nfor t in range (len(Home_team)):\\n    if t%500==0:\\n        print(t, \"/\", len(Home_team))\\n    new_state = get_state(Home_team, Away_team, Ball_team, average_compo_home, average_compo_away, t, Delta)\\n    States.append(new_state)\\n    state = new_state\\nStates = torch.stack(States)\\n\\ntorch.save(States, \\'States.pt\\')\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Compute the states\n",
    "Delta = 1\n",
    "Mt = np.zeros((23, 23))\n",
    "Mr1 = np.zeros((23, 23))\n",
    "Mr2 = np.zeros((23, 23))\n",
    "Mp1 = np.zeros((23, 23))\n",
    "Mp2 = np.zeros((23, 23))\n",
    "Mv1 = np.zeros((23, 23))\n",
    "Mv2 = np.zeros((23, 23))\n",
    "Ma1 = np.zeros((23, 23))\n",
    "Ma2 = np.zeros((23, 23))\n",
    "state = np.stack([Mt,Mr1, Mr2, Mp1, Mp2, Mv1, Mv2, Ma1, Ma2], axis = 2)\n",
    "States = []\n",
    "for t in range (len(Home_team)):\n",
    "    if t%500==0:\n",
    "        print(t, \"/\", len(Home_team))\n",
    "    new_state = get_state(Home_team, Away_team, Ball_team, average_compo_home, average_compo_away, t, Delta)\n",
    "    States.append(new_state)\n",
    "    state = new_state\n",
    "States = torch.stack(States)\n",
    "\n",
    "torch.save(States, 'States.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "898b24f4-797c-471c-ae5d-8e49c2d2c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "average_compo_home = torch.load('average_compo_home.pt')\n",
    "average_compo_away =  torch.load('average_compo_away.pt')\n",
    "States = torch.load('States.pt')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8038f56d-97f6-46a1-a483-c08a687174d7",
   "metadata": {},
   "source": [
    "Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad9bfac0-65fe-4caa-a348-a7be9e8e27a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3364\\1063380874.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 22.368934390309093\n",
      "Epoch 2/15, Loss: 20.496269194634404\n",
      "Epoch 3/15, Loss: 19.66093844109839\n",
      "Epoch 4/15, Loss: 19.072323945852425\n",
      "Epoch 5/15, Loss: 18.57374625153594\n",
      "Epoch 6/15, Loss: 18.13703979240669\n",
      "Epoch 7/15, Loss: 17.7287781893552\n",
      "Epoch 8/15, Loss: 17.33902326520983\n",
      "Epoch 9/15, Loss: 17.001966162042304\n",
      "Epoch 10/15, Loss: 16.676660710638696\n",
      "Epoch 11/15, Loss: 16.310682747390246\n",
      "Epoch 12/15, Loss: 15.9401720225156\n",
      "Epoch 13/15, Loss: 15.626561971811148\n",
      "Epoch 14/15, Loss: 15.329637013948881\n",
      "Epoch 15/15, Loss: 15.012875593625582\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "data_loader = DataLoader(States, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "         # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(9, 32, kernel_size=3, stride=1, padding=1),  # Input size: 9x23x23, Output size: 32x23x23\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), # Input size: 32x23x23, Output size: 64x23x23\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                # Input size: 64x23x23, Output size: 64x11x11\n",
    "            nn.Flatten(),                                         # Flatten the output into a vector\n",
    "            nn.Linear(64*11*11, 128),                             # Fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)                                    # Output layer with 10 units\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, 128),                       # Input size: 1x10, Output size: 1x128\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64*11*11),                 # Fully connected layer to match desired shape\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, 11, 11)),            # Reshape the output into a 3D tensor: 64x11x11\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2), # Transposed convolutional layer: 32x23x23\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 9, kernel_size=3, stride=1, padding=0), # Transposed convolutional layer: 9x23x23\n",
    "            nn.ReLU(),\n",
    "            nn.ZeroPad2d((0, -1, 0, -1))  # Crop the output to match input size\n",
    "        )        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "        \n",
    "   \n",
    "model = ConvAutoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "Encoded_States = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    # Inside the training loop\n",
    "    for data in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_data = data \n",
    "        data = torch.tensor(data)\n",
    "\n",
    "        #save encoded states for the LSTM\n",
    "        encoded =  model.encode(data)\n",
    "        for d in encoded:\n",
    "            Encoded_States.append(d)\n",
    "            \n",
    "        reconstructions= model(input_data)\n",
    "        loss = criterion(reconstructions, input_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(data_loader)}\")\n",
    "\n",
    "   \n",
    "print('Finished Training')\n",
    "torch.save(Encoded_States, 'Encoded_States.pt') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc5e839-ed89-48f0-bcb0-bb3f630f4de3",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca4d25e7-5361-409d-9064-3985008817f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3364\\3686844713.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vel_tensor = torch.tensor(vel)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3364\\3686844713.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  acc_tensor = torch.tensor(acc)\n"
     ]
    }
   ],
   "source": [
    "#Create Dataset\n",
    "Datasets = []\n",
    "Delta = 1\n",
    "n = len(Home_team)\n",
    "T = 10\n",
    "m = int(torch.floor(torch.tensor(n/T)))\n",
    "#n=5\n",
    "\n",
    "for p in range(23):\n",
    "    X = torch.zeros(m, 10, 17)\n",
    "    Y = torch.zeros(m, 2)\n",
    "    D = torch.zeros(10,17)\n",
    "    i = 0\n",
    "    for t in range(n):\n",
    "        if i==T:\n",
    "            X[int(torch.floor(torch.tensor(t/T)))] = D\n",
    "            Y[int(torch.floor(torch.tensor(t/T)))] = pos_tensor\n",
    "            i = 0\n",
    "        else:\n",
    "            frame = Home_team[t] + Away_team[t] + [Ball_team[t]]\n",
    "            rel_pos = relative_positions(frame, True)\n",
    "            if p<=10:\n",
    "                index_player, team_label, Role_team, pos, vel, acc = get_features(p, Home_team, average_compo_home, rel_pos, t, Delta)\n",
    "            elif p<=21:\n",
    "                index_player, team_label, Role_team, pos, vel, acc = get_features(p, Away_team, average_compo_home, rel_pos, t, Delta)\n",
    "            else:\n",
    "                index_player, team_label, Role_team, pos, vel, acc = get_features(23, Ball_team, average_compo_home, rel_pos, t, Delta)\n",
    "            pos_tensor = torch.tensor(pos)\n",
    "            vel_tensor = torch.tensor(vel)\n",
    "            acc_tensor = torch.tensor(acc)\n",
    "            team_label = [team_label]\n",
    "            team_tensor = torch.tensor(team_label)\n",
    "            features = torch.cat((pos_tensor, vel_tensor, acc_tensor, team_tensor))\n",
    "            input = torch.cat((features, Encoded_States[t]))\n",
    "            D[i] = input\n",
    "            i+=1\n",
    "    Datasets.append([X,Y])\n",
    "\n",
    "# Define the percentage of data for training and testing\n",
    "train_ratio = 0.8  \n",
    "\n",
    "# Initialize lists to store the split datasets\n",
    "split_datasets = []\n",
    "\n",
    "# Loop over each dataset\n",
    "for X, Y in Datasets:\n",
    "    # Determine the number of samples for training and testing\n",
    "    num_samples = X.size(0)\n",
    "    num_train = int(train_ratio * num_samples)\n",
    "    num_test = num_samples - num_train\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test = X[:num_train], X[num_train:]\n",
    "    y_train, y_test = Y[:num_train], Y[num_train:]\n",
    "    \n",
    "    # Append the split dataset to the list\n",
    "    split_datasets.append((X_train, y_train, X_test, y_test))\n",
    "torch.save(split_datasets, 'split_datasets.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d98a3-17ee-4092-b1e0-9bea2e053303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "split_datasets = torch.load('split_datasets.pt')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "747729f8-667d-4ac7-b5c9-a549e36ec798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3364\\2822586726.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_tensor = torch.tensor(X_train).float()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3364\\2822586726.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train_tensor = torch.tensor(y_train).float()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3364\\2822586726.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test).float()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3364\\2822586726.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_tensor = torch.tensor(y_test).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200], Loss: 2148.3388\n",
      "Epoch [10/200], Loss: 827.1777\n",
      "Epoch [20/200], Loss: 310.9864\n",
      "Epoch [30/200], Loss: 144.2034\n",
      "Epoch [40/200], Loss: 81.0035\n",
      "Epoch [50/200], Loss: 47.6100\n",
      "Epoch [60/200], Loss: 42.9464\n",
      "Epoch [70/200], Loss: 38.8891\n",
      "Epoch [80/200], Loss: 35.2047\n",
      "Epoch [90/200], Loss: 31.7666\n",
      "Epoch [100/200], Loss: 28.8495\n",
      "Epoch [110/200], Loss: 28.5225\n",
      "Epoch [120/200], Loss: 28.2153\n",
      "Epoch [130/200], Loss: 27.8947\n",
      "Epoch [140/200], Loss: 27.5804\n",
      "Epoch [150/200], Loss: 27.2779\n",
      "Epoch [160/200], Loss: 27.2463\n",
      "Epoch [170/200], Loss: 27.2139\n",
      "Epoch [180/200], Loss: 27.1823\n",
      "Epoch [190/200], Loss: 27.1502\n",
      "Model 1 - Test Loss: 36.3433\n",
      "Epoch [0/200], Loss: 568.7514\n",
      "Epoch [10/200], Loss: 71.1043\n",
      "Epoch [20/200], Loss: 18.3242\n",
      "Epoch [30/200], Loss: 5.4721\n",
      "Epoch [40/200], Loss: 1.8768\n",
      "Epoch [50/200], Loss: 0.8390\n",
      "Epoch [60/200], Loss: 0.6455\n",
      "Epoch [70/200], Loss: 0.5339\n",
      "Epoch [80/200], Loss: 0.4451\n",
      "Epoch [90/200], Loss: 0.3776\n",
      "Epoch [100/200], Loss: 0.3178\n",
      "Epoch [110/200], Loss: 0.3094\n",
      "Epoch [120/200], Loss: 0.3028\n",
      "Epoch [130/200], Loss: 0.2963\n",
      "Epoch [140/200], Loss: 0.2901\n",
      "Epoch [150/200], Loss: 0.2836\n",
      "Epoch [160/200], Loss: 0.2829\n",
      "Epoch [170/200], Loss: 0.2823\n",
      "Epoch [180/200], Loss: 0.2816\n",
      "Epoch [190/200], Loss: 0.2809\n",
      "Model 3 - Test Loss: 2.2410\n",
      "Epoch [0/200], Loss: 1797.2299\n",
      "Epoch [10/200], Loss: 644.1918\n",
      "Epoch [20/200], Loss: 211.5030\n",
      "Epoch [30/200], Loss: 101.6279\n",
      "Epoch [40/200], Loss: 60.1921\n",
      "Epoch [50/200], Loss: 35.6830\n",
      "Epoch [60/200], Loss: 32.0478\n",
      "Epoch [70/200], Loss: 28.9478\n",
      "Epoch [80/200], Loss: 26.0688\n",
      "Epoch [90/200], Loss: 23.3964\n",
      "Epoch [100/200], Loss: 21.0439\n",
      "Epoch [110/200], Loss: 20.7797\n",
      "Epoch [120/200], Loss: 20.5279\n",
      "Epoch [130/200], Loss: 20.2711\n",
      "Epoch [140/200], Loss: 20.0174\n",
      "Epoch [150/200], Loss: 19.7712\n",
      "Epoch [160/200], Loss: 19.7455\n",
      "Epoch [170/200], Loss: 19.7192\n",
      "Epoch [180/200], Loss: 19.6922\n",
      "Epoch [190/200], Loss: 19.6666\n",
      "Model 4 - Test Loss: 9.7951\n",
      "Epoch [0/200], Loss: 1583.0179\n",
      "Epoch [10/200], Loss: 493.5480\n",
      "Epoch [20/200], Loss: 180.6436\n",
      "Epoch [30/200], Loss: 106.3276\n",
      "Epoch [40/200], Loss: 67.1357\n",
      "Epoch [50/200], Loss: 42.7446\n",
      "Epoch [60/200], Loss: 38.7125\n",
      "Epoch [70/200], Loss: 35.3525\n",
      "Epoch [80/200], Loss: 31.9779\n",
      "Epoch [90/200], Loss: 28.9124\n",
      "Epoch [100/200], Loss: 26.1841\n",
      "Epoch [110/200], Loss: 25.8879\n",
      "Epoch [120/200], Loss: 25.5973\n",
      "Epoch [130/200], Loss: 25.3099\n",
      "Epoch [140/200], Loss: 25.0174\n",
      "Epoch [150/200], Loss: 24.7462\n",
      "Epoch [160/200], Loss: 24.7182\n",
      "Epoch [170/200], Loss: 24.6898\n",
      "Epoch [180/200], Loss: 24.6606\n",
      "Epoch [190/200], Loss: 24.6324\n",
      "Model 11 - Test Loss: 6.3323\n",
      "Epoch [0/200], Loss: 1616.7145\n",
      "Epoch [10/200], Loss: 524.1317\n",
      "Epoch [20/200], Loss: 212.9346\n",
      "Epoch [30/200], Loss: 124.4234\n",
      "Epoch [40/200], Loss: 71.4720\n",
      "Epoch [50/200], Loss: 37.9842\n",
      "Epoch [60/200], Loss: 33.9185\n",
      "Epoch [70/200], Loss: 30.2470\n",
      "Epoch [80/200], Loss: 27.0527\n",
      "Epoch [90/200], Loss: 24.3107\n",
      "Epoch [100/200], Loss: 21.9336\n",
      "Epoch [110/200], Loss: 21.6688\n",
      "Epoch [120/200], Loss: 21.4058\n",
      "Epoch [130/200], Loss: 21.1499\n",
      "Epoch [140/200], Loss: 20.8917\n",
      "Epoch [150/200], Loss: 20.6465\n",
      "Epoch [160/200], Loss: 20.6210\n",
      "Epoch [170/200], Loss: 20.5954\n",
      "Epoch [180/200], Loss: 20.5700\n",
      "Epoch [190/200], Loss: 20.5440\n",
      "Model 17 - Test Loss: 30.3058\n",
      "Epoch [0/200], Loss: 1804.4831\n",
      "Epoch [10/200], Loss: 690.6095\n",
      "Epoch [20/200], Loss: 292.9083\n",
      "Epoch [30/200], Loss: 129.4558\n",
      "Epoch [40/200], Loss: 61.0888\n",
      "Epoch [50/200], Loss: 30.7851\n",
      "Epoch [60/200], Loss: 27.0170\n",
      "Epoch [70/200], Loss: 23.9064\n",
      "Epoch [80/200], Loss: 21.1950\n",
      "Epoch [90/200], Loss: 18.7963\n",
      "Epoch [100/200], Loss: 16.8225\n",
      "Epoch [110/200], Loss: 16.5995\n",
      "Epoch [120/200], Loss: 16.3864\n",
      "Epoch [130/200], Loss: 16.1740\n",
      "Epoch [140/200], Loss: 15.9550\n",
      "Epoch [150/200], Loss: 15.7478\n",
      "Epoch [160/200], Loss: 15.7260\n",
      "Epoch [170/200], Loss: 15.7044\n",
      "Epoch [180/200], Loss: 15.6827\n",
      "Epoch [190/200], Loss: 15.6608\n",
      "Model 14 - Test Loss: 8.9316\n",
      "Epoch [0/200], Loss: 2157.2072\n",
      "Epoch [10/200], Loss: 787.4000\n",
      "Epoch [20/200], Loss: 275.2311\n",
      "Epoch [30/200], Loss: 141.6358\n",
      "Epoch [40/200], Loss: 90.3946\n",
      "Epoch [50/200], Loss: 55.5434\n",
      "Epoch [60/200], Loss: 50.8839\n",
      "Epoch [70/200], Loss: 46.5348\n",
      "Epoch [80/200], Loss: 42.6386\n",
      "Epoch [90/200], Loss: 38.9113\n",
      "Epoch [100/200], Loss: 35.6756\n",
      "Epoch [110/200], Loss: 35.3129\n",
      "Epoch [120/200], Loss: 34.9566\n",
      "Epoch [130/200], Loss: 34.6004\n",
      "Epoch [140/200], Loss: 34.2533\n",
      "Epoch [150/200], Loss: 33.9170\n",
      "Epoch [160/200], Loss: 33.8810\n",
      "Epoch [170/200], Loss: 33.8460\n",
      "Epoch [180/200], Loss: 33.8096\n",
      "Epoch [190/200], Loss: 33.7734\n",
      "Model 12 - Test Loss: 26.2912\n",
      "Epoch [0/200], Loss: 1958.0121\n",
      "Epoch [10/200], Loss: 768.3275\n",
      "Epoch [20/200], Loss: 292.5191\n",
      "Epoch [30/200], Loss: 142.9182\n",
      "Epoch [40/200], Loss: 79.2855\n",
      "Epoch [50/200], Loss: 44.7886\n",
      "Epoch [60/200], Loss: 40.5144\n",
      "Epoch [70/200], Loss: 36.6872\n",
      "Epoch [80/200], Loss: 33.3306\n",
      "Epoch [90/200], Loss: 30.2848\n",
      "Epoch [100/200], Loss: 27.5486\n",
      "Epoch [110/200], Loss: 27.2501\n",
      "Epoch [120/200], Loss: 26.9662\n",
      "Epoch [130/200], Loss: 26.6771\n",
      "Epoch [140/200], Loss: 26.3933\n",
      "Epoch [150/200], Loss: 26.1283\n",
      "Epoch [160/200], Loss: 26.1005\n",
      "Epoch [170/200], Loss: 26.0723\n",
      "Epoch [180/200], Loss: 26.0435\n",
      "Epoch [190/200], Loss: 26.0154\n",
      "Model 21 - Test Loss: 22.5988\n",
      "Epoch [0/200], Loss: 2262.4990\n",
      "Epoch [10/200], Loss: 916.8916\n",
      "Epoch [20/200], Loss: 374.5313\n",
      "Epoch [30/200], Loss: 197.7230\n",
      "Epoch [40/200], Loss: 115.1386\n",
      "Epoch [50/200], Loss: 62.0082\n",
      "Epoch [60/200], Loss: 55.3504\n",
      "Epoch [70/200], Loss: 49.3219\n",
      "Epoch [80/200], Loss: 43.8890\n",
      "Epoch [90/200], Loss: 39.0730\n",
      "Epoch [100/200], Loss: 34.9094\n",
      "Epoch [110/200], Loss: 34.4585\n",
      "Epoch [120/200], Loss: 34.0109\n",
      "Epoch [130/200], Loss: 33.5616\n",
      "Epoch [140/200], Loss: 33.1169\n",
      "Epoch [150/200], Loss: 32.7038\n",
      "Epoch [160/200], Loss: 32.6588\n",
      "Epoch [170/200], Loss: 32.6145\n",
      "Epoch [180/200], Loss: 32.5708\n",
      "Epoch [190/200], Loss: 32.5268\n",
      "Model 23 - Test Loss: 15.2741\n",
      "Epoch [0/200], Loss: 2478.5501\n",
      "Epoch [10/200], Loss: 1035.7236\n",
      "Epoch [20/200], Loss: 441.0789\n",
      "Epoch [30/200], Loss: 198.6217\n",
      "Epoch [40/200], Loss: 96.5130\n",
      "Epoch [50/200], Loss: 47.3212\n",
      "Epoch [60/200], Loss: 41.9332\n",
      "Epoch [70/200], Loss: 37.1038\n",
      "Epoch [80/200], Loss: 33.1356\n",
      "Epoch [90/200], Loss: 29.6020\n",
      "Epoch [100/200], Loss: 26.7071\n",
      "Epoch [110/200], Loss: 26.3932\n",
      "Epoch [120/200], Loss: 26.0759\n",
      "Epoch [130/200], Loss: 25.7567\n",
      "Epoch [140/200], Loss: 25.4422\n",
      "Epoch [150/200], Loss: 25.1393\n",
      "Epoch [160/200], Loss: 25.1062\n",
      "Epoch [170/200], Loss: 25.0744\n",
      "Epoch [180/200], Loss: 25.0419\n",
      "Epoch [190/200], Loss: 25.0080\n",
      "Model 25 - Test Loss: 16.4669\n",
      "Epoch [0/200], Loss: 2593.3520\n",
      "Epoch [10/200], Loss: 1130.0338\n",
      "Epoch [20/200], Loss: 463.3836\n",
      "Epoch [30/200], Loss: 211.2643\n",
      "Epoch [40/200], Loss: 107.9957\n",
      "Epoch [50/200], Loss: 61.7212\n",
      "Epoch [60/200], Loss: 56.2837\n",
      "Epoch [70/200], Loss: 51.2997\n",
      "Epoch [80/200], Loss: 46.5727\n",
      "Epoch [90/200], Loss: 42.3219\n",
      "Epoch [100/200], Loss: 38.4089\n",
      "Epoch [110/200], Loss: 37.9882\n",
      "Epoch [120/200], Loss: 37.5851\n",
      "Epoch [130/200], Loss: 37.1787\n",
      "Epoch [140/200], Loss: 36.7694\n",
      "Epoch [150/200], Loss: 36.3750\n",
      "Epoch [160/200], Loss: 36.3339\n",
      "Epoch [170/200], Loss: 36.2920\n",
      "Epoch [180/200], Loss: 36.2500\n",
      "Epoch [190/200], Loss: 36.2077\n",
      "Model 10 - Test Loss: 22.6076\n",
      "Epoch [0/200], Loss: 22.8431\n",
      "Epoch [10/200], Loss: 6.2771\n",
      "Epoch [20/200], Loss: 2.5475\n",
      "Epoch [30/200], Loss: 1.3340\n",
      "Epoch [40/200], Loss: 0.7871\n",
      "Epoch [50/200], Loss: 0.4535\n",
      "Epoch [60/200], Loss: 0.1929\n",
      "Epoch [70/200], Loss: 0.1506\n",
      "Epoch [80/200], Loss: 0.1153\n",
      "Epoch [90/200], Loss: 0.0931\n",
      "Epoch [100/200], Loss: 0.0710\n",
      "Epoch [110/200], Loss: 0.0647\n",
      "Epoch [120/200], Loss: 0.0620\n",
      "Epoch [130/200], Loss: 0.0596\n",
      "Epoch [140/200], Loss: 0.0576\n",
      "Epoch [150/200], Loss: 0.0554\n",
      "Epoch [160/200], Loss: 0.0550\n",
      "Epoch [170/200], Loss: 0.0548\n",
      "Epoch [180/200], Loss: 0.0546\n",
      "Epoch [190/200], Loss: 0.0544\n",
      "Model 1 - Test Loss: 8.7637\n",
      "Epoch [0/200], Loss: 4392.3000\n",
      "Epoch [10/200], Loss: 2407.1639\n",
      "Epoch [20/200], Loss: 1206.8407\n",
      "Epoch [30/200], Loss: 486.0343\n",
      "Epoch [40/200], Loss: 113.0438\n",
      "Epoch [50/200], Loss: 38.1462\n",
      "Epoch [60/200], Loss: 35.5975\n",
      "Epoch [70/200], Loss: 33.2211\n",
      "Epoch [80/200], Loss: 29.8454\n",
      "Epoch [90/200], Loss: 27.4545\n",
      "Epoch [100/200], Loss: 25.0669\n",
      "Epoch [110/200], Loss: 24.8066\n",
      "Epoch [120/200], Loss: 24.5691\n",
      "Epoch [130/200], Loss: 24.3216\n",
      "Epoch [140/200], Loss: 24.0782\n",
      "Epoch [150/200], Loss: 23.8306\n",
      "Epoch [160/200], Loss: 23.8040\n",
      "Epoch [170/200], Loss: 23.7773\n",
      "Epoch [180/200], Loss: 23.7504\n",
      "Epoch [190/200], Loss: 23.7221\n",
      "Model 5 - Test Loss: 20.8302\n",
      "Epoch [0/200], Loss: 97.1803\n",
      "Epoch [10/200], Loss: 13.8498\n",
      "Epoch [20/200], Loss: 4.2949\n",
      "Epoch [30/200], Loss: 2.3127\n",
      "Epoch [40/200], Loss: 1.4591\n",
      "Epoch [50/200], Loss: 0.9312\n",
      "Epoch [60/200], Loss: 0.6838\n",
      "Epoch [70/200], Loss: 0.5996\n",
      "Epoch [80/200], Loss: 0.5270\n",
      "Epoch [90/200], Loss: 0.4618\n",
      "Epoch [100/200], Loss: 0.4018\n",
      "Epoch [110/200], Loss: 0.3911\n",
      "Epoch [120/200], Loss: 0.3846\n",
      "Epoch [130/200], Loss: 0.3780\n",
      "Epoch [140/200], Loss: 0.3718\n",
      "Epoch [150/200], Loss: 0.3650\n",
      "Epoch [160/200], Loss: 0.3643\n",
      "Epoch [170/200], Loss: 0.3636\n",
      "Epoch [180/200], Loss: 0.3630\n",
      "Epoch [190/200], Loss: 0.3624\n",
      "Model 4 - Test Loss: 5.1361\n",
      "Epoch [0/200], Loss: 84.1063\n",
      "Epoch [10/200], Loss: 12.6307\n",
      "Epoch [20/200], Loss: 5.4878\n",
      "Epoch [30/200], Loss: 3.0933\n",
      "Epoch [40/200], Loss: 1.9196\n",
      "Epoch [50/200], Loss: 1.2456\n",
      "Epoch [60/200], Loss: 0.9483\n",
      "Epoch [70/200], Loss: 0.8256\n",
      "Epoch [80/200], Loss: 0.7073\n",
      "Epoch [90/200], Loss: 0.6132\n",
      "Epoch [100/200], Loss: 0.5338\n",
      "Epoch [110/200], Loss: 0.5160\n",
      "Epoch [120/200], Loss: 0.5068\n",
      "Epoch [130/200], Loss: 0.4977\n",
      "Epoch [140/200], Loss: 0.4888\n",
      "Epoch [150/200], Loss: 0.4798\n",
      "Epoch [160/200], Loss: 0.4789\n",
      "Epoch [170/200], Loss: 0.4780\n",
      "Epoch [180/200], Loss: 0.4771\n",
      "Epoch [190/200], Loss: 0.4763\n",
      "Model 11 - Test Loss: 3.9599\n",
      "Epoch [0/200], Loss: 3061.6478\n",
      "Epoch [10/200], Loss: 1379.0916\n",
      "Epoch [20/200], Loss: 533.0190\n",
      "Epoch [30/200], Loss: 212.3273\n",
      "Epoch [40/200], Loss: 111.5097\n",
      "Epoch [50/200], Loss: 62.5352\n",
      "Epoch [60/200], Loss: 56.1675\n",
      "Epoch [70/200], Loss: 50.8799\n",
      "Epoch [80/200], Loss: 45.9787\n",
      "Epoch [90/200], Loss: 41.1305\n",
      "Epoch [100/200], Loss: 36.8469\n",
      "Epoch [110/200], Loss: 36.3712\n",
      "Epoch [120/200], Loss: 35.9085\n",
      "Epoch [130/200], Loss: 35.4336\n",
      "Epoch [140/200], Loss: 34.9744\n",
      "Epoch [150/200], Loss: 34.5276\n",
      "Epoch [160/200], Loss: 34.4808\n",
      "Epoch [170/200], Loss: 34.4342\n",
      "Epoch [180/200], Loss: 34.3888\n",
      "Epoch [190/200], Loss: 34.3411\n",
      "Model 15 - Test Loss: 33.0711\n",
      "Epoch [0/200], Loss: 1881.3598\n",
      "Epoch [10/200], Loss: 761.3899\n",
      "Epoch [20/200], Loss: 315.4967\n",
      "Epoch [30/200], Loss: 135.9740\n",
      "Epoch [40/200], Loss: 61.1495\n",
      "Epoch [50/200], Loss: 28.5450\n",
      "Epoch [60/200], Loss: 24.8016\n",
      "Epoch [70/200], Loss: 21.6563\n",
      "Epoch [80/200], Loss: 18.8806\n",
      "Epoch [90/200], Loss: 16.4295\n",
      "Epoch [100/200], Loss: 14.4283\n",
      "Epoch [110/200], Loss: 14.2022\n",
      "Epoch [120/200], Loss: 13.9813\n",
      "Epoch [130/200], Loss: 13.7706\n",
      "Epoch [140/200], Loss: 13.5536\n",
      "Epoch [150/200], Loss: 13.3521\n",
      "Epoch [160/200], Loss: 13.3309\n",
      "Epoch [170/200], Loss: 13.3096\n",
      "Epoch [180/200], Loss: 13.2877\n",
      "Epoch [190/200], Loss: 13.2664\n",
      "Model 8 - Test Loss: 21.6517\n",
      "Epoch [0/200], Loss: 19.2706\n",
      "Epoch [10/200], Loss: 4.5165\n",
      "Epoch [20/200], Loss: 2.3324\n",
      "Epoch [30/200], Loss: 1.3032\n",
      "Epoch [40/200], Loss: 0.7895\n",
      "Epoch [50/200], Loss: 0.5083\n",
      "Epoch [60/200], Loss: 0.2318\n",
      "Epoch [70/200], Loss: 0.1675\n",
      "Epoch [80/200], Loss: 0.1268\n",
      "Epoch [90/200], Loss: 0.0953\n",
      "Epoch [100/200], Loss: 0.0715\n",
      "Epoch [110/200], Loss: 0.0654\n",
      "Epoch [120/200], Loss: 0.0626\n",
      "Epoch [130/200], Loss: 0.0599\n",
      "Epoch [140/200], Loss: 0.0572\n",
      "Epoch [150/200], Loss: 0.0547\n",
      "Epoch [160/200], Loss: 0.0543\n",
      "Epoch [170/200], Loss: 0.0541\n",
      "Epoch [180/200], Loss: 0.0539\n",
      "Epoch [190/200], Loss: 0.0536\n",
      "Model 21 - Test Loss: 4.2763\n",
      "Epoch [0/200], Loss: 2501.7754\n",
      "Epoch [10/200], Loss: 1087.7125\n",
      "Epoch [20/200], Loss: 424.8577\n",
      "Epoch [30/200], Loss: 163.7084\n",
      "Epoch [40/200], Loss: 73.3599\n",
      "Epoch [50/200], Loss: 37.7600\n",
      "Epoch [60/200], Loss: 33.1830\n",
      "Epoch [70/200], Loss: 29.2546\n",
      "Epoch [80/200], Loss: 25.7514\n",
      "Epoch [90/200], Loss: 22.6220\n",
      "Epoch [100/200], Loss: 19.9705\n",
      "Epoch [110/200], Loss: 19.6846\n",
      "Epoch [120/200], Loss: 19.4067\n",
      "Epoch [130/200], Loss: 19.1328\n",
      "Epoch [140/200], Loss: 18.8561\n",
      "Epoch [150/200], Loss: 18.5889\n",
      "Epoch [160/200], Loss: 18.5604\n",
      "Epoch [170/200], Loss: 18.5315\n",
      "Epoch [180/200], Loss: 18.5029\n",
      "Epoch [190/200], Loss: 18.4740\n",
      "Model 19 - Test Loss: 22.4574\n",
      "Epoch [0/200], Loss: 26.0031\n",
      "Epoch [10/200], Loss: 5.8646\n",
      "Epoch [20/200], Loss: 2.6262\n",
      "Epoch [30/200], Loss: 1.2693\n",
      "Epoch [40/200], Loss: 0.8750\n",
      "Epoch [50/200], Loss: 0.6098\n",
      "Epoch [60/200], Loss: 0.2312\n",
      "Epoch [70/200], Loss: 0.1782\n",
      "Epoch [80/200], Loss: 0.1473\n",
      "Epoch [90/200], Loss: 0.1186\n",
      "Epoch [100/200], Loss: 0.0961\n",
      "Epoch [110/200], Loss: 0.0882\n",
      "Epoch [120/200], Loss: 0.0850\n",
      "Epoch [130/200], Loss: 0.0822\n",
      "Epoch [140/200], Loss: 0.0796\n",
      "Epoch [150/200], Loss: 0.0768\n",
      "Epoch [160/200], Loss: 0.0765\n",
      "Epoch [170/200], Loss: 0.0762\n",
      "Epoch [180/200], Loss: 0.0760\n",
      "Epoch [190/200], Loss: 0.0757\n",
      "Model 23 - Test Loss: 7.6956\n",
      "Epoch [0/200], Loss: 57.7486\n",
      "Epoch [10/200], Loss: 12.6982\n",
      "Epoch [20/200], Loss: 4.4375\n",
      "Epoch [30/200], Loss: 2.2933\n",
      "Epoch [40/200], Loss: 1.4686\n",
      "Epoch [50/200], Loss: 0.8679\n",
      "Epoch [60/200], Loss: 0.5171\n",
      "Epoch [70/200], Loss: 0.4396\n",
      "Epoch [80/200], Loss: 0.3791\n",
      "Epoch [90/200], Loss: 0.3262\n",
      "Epoch [100/200], Loss: 0.2731\n",
      "Epoch [110/200], Loss: 0.2616\n",
      "Epoch [120/200], Loss: 0.2557\n",
      "Epoch [130/200], Loss: 0.2498\n",
      "Epoch [140/200], Loss: 0.2445\n",
      "Epoch [150/200], Loss: 0.2385\n",
      "Epoch [160/200], Loss: 0.2379\n",
      "Epoch [170/200], Loss: 0.2373\n",
      "Epoch [180/200], Loss: 0.2368\n",
      "Epoch [190/200], Loss: 0.2362\n",
      "Model 12 - Test Loss: 9.5134\n",
      "Epoch [0/200], Loss: 1616.8074\n",
      "Epoch [10/200], Loss: 552.9427\n",
      "Epoch [20/200], Loss: 180.6099\n",
      "Epoch [30/200], Loss: 86.4422\n",
      "Epoch [40/200], Loss: 42.7299\n",
      "Epoch [50/200], Loss: 21.9705\n",
      "Epoch [60/200], Loss: 19.1827\n",
      "Epoch [70/200], Loss: 16.7991\n",
      "Epoch [80/200], Loss: 14.6342\n",
      "Epoch [90/200], Loss: 12.7586\n",
      "Epoch [100/200], Loss: 11.2264\n",
      "Epoch [110/200], Loss: 11.0470\n",
      "Epoch [120/200], Loss: 10.8726\n",
      "Epoch [130/200], Loss: 10.7039\n",
      "Epoch [140/200], Loss: 10.5387\n",
      "Epoch [150/200], Loss: 10.3802\n",
      "Epoch [160/200], Loss: 10.3634\n",
      "Epoch [170/200], Loss: 10.3467\n",
      "Epoch [180/200], Loss: 10.3296\n",
      "Epoch [190/200], Loss: 10.3129\n",
      "Model 22 - Test Loss: 32.1252\n",
      "Epoch [0/200], Loss: 2008.9731\n",
      "Epoch [10/200], Loss: 783.1377\n",
      "Epoch [20/200], Loss: 335.1962\n",
      "Epoch [30/200], Loss: 190.8638\n",
      "Epoch [40/200], Loss: 114.3877\n",
      "Epoch [50/200], Loss: 66.4754\n",
      "Epoch [60/200], Loss: 60.6405\n",
      "Epoch [70/200], Loss: 55.5738\n",
      "Epoch [80/200], Loss: 50.8240\n",
      "Epoch [90/200], Loss: 46.5300\n",
      "Epoch [100/200], Loss: 42.6787\n",
      "Epoch [110/200], Loss: 42.2660\n",
      "Epoch [120/200], Loss: 41.8593\n",
      "Epoch [130/200], Loss: 41.4494\n",
      "Epoch [140/200], Loss: 41.0355\n",
      "Epoch [150/200], Loss: 40.6262\n",
      "Epoch [160/200], Loss: 40.5841\n",
      "Epoch [170/200], Loss: 40.5421\n",
      "Epoch [180/200], Loss: 40.4997\n",
      "Epoch [190/200], Loss: 40.4568\n",
      "Model 27 - Test Loss: 52.5597\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters\n",
    "input_dim = 17\n",
    "output_dim = 2\n",
    "num_units = 128\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "sequence_length = 10\n",
    "\n",
    "# Define the LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_units, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, num_units, batch_first=True) \n",
    "        self.fc = nn.Linear(num_units, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize and save all models\n",
    "for i in range(27):\n",
    "    model = LSTMModel(input_dim, num_units, output_dim)\n",
    "    torch.save(model.state_dict(), f'lstm_model_{i+1}.pt')\n",
    "\n",
    "# Loop to train and evaluate models\n",
    "for i, data in enumerate(split_datasets):\n",
    "    X_train, y_train, X_test, y_test = data\n",
    "    \n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train).float()\n",
    "    y_train_tensor = torch.tensor(y_train).float()\n",
    "    X_test_tensor = torch.tensor(X_test).float()\n",
    "    y_test_tensor = torch.tensor(y_test).float()\n",
    "    \n",
    "    # Create DataLoader for training and testing\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size)\n",
    "\n",
    "    # Find corresponding model\n",
    "    k = i+1\n",
    "    if k == 1 or k==12:\n",
    "        m = 1\n",
    "    elif k > 1 and k < 12:\n",
    "        role = find_element(average_compo_home, k)\n",
    "        m = place_in_compo(role)\n",
    "    elif k > 11 and k < 23:\n",
    "        role = find_element(average_compo_away, k-11)\n",
    "        \n",
    "        m = place_in_compo(role)   \n",
    "    else:\n",
    "        m = 27\n",
    "\n",
    "    # Load the model\n",
    "    model = LSTMModel(input_dim, num_units, output_dim)\n",
    "    model.load_state_dict(torch.load(f'lstm_model_{m}.pt'))\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), f'lstm_model_{m}.pt')\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, targets).item() * inputs.size(0)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Model {m} - Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23559323-af83-4a62-b871-2f7a485ed834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
